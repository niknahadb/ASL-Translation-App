{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7991150d-d635-46c0-89c7-ac5a2bcae219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from VideoLoader import KeypointExtractor, read_video\n",
    "from VideoDataset import process_keypoints\n",
    "from model import SLR\n",
    "\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87d40cf3-cdc2-4844-8882-f7d5901bdfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def record_video_tensor(prep_time=3, record_time=3, fps=30):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(\"Cannot access the webcam\")\n",
    "\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    print(f\"Preparing to record at {width}x{height}, {fps} FPS...\")\n",
    "\n",
    "    # Preparation countdown\n",
    "    start_prep = time.time()\n",
    "    while time.time() - start_prep < prep_time:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "\n",
    "        # Countdown overlay\n",
    "        time_left = prep_time - int(time.time() - start_prep)\n",
    "        cv2.putText(frame, f\"Recording in {time_left}\", (50, 100),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 4)\n",
    "\n",
    "        cv2.imshow(\"Preview\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            return None\n",
    "\n",
    "    # Start recording\n",
    "    print(\"Recording started!\")\n",
    "    num_frames = int(record_time * fps)\n",
    "    frames = []\n",
    "    start_record = time.time()\n",
    "\n",
    "    while len(frames) < num_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Time remaining\n",
    "        elapsed = time.time() - start_record\n",
    "        remaining = max(0, record_time - elapsed)\n",
    "        cv2.putText(frame, f\"{remaining:.1f}s left\", (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2)\n",
    "\n",
    "        # Show frame\n",
    "        cv2.imshow(\"Preview\", frame)\n",
    "\n",
    "        # Convert and store\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(frame_rgb)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "        # FPS control\n",
    "        expected = len(frames) / fps\n",
    "        if expected > elapsed:\n",
    "            time.sleep(expected - elapsed)\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    video_np = np.stack(frames)\n",
    "    video_tensor = torch.from_numpy(video_np).float()\n",
    "\n",
    "    print(f\"Captured video tensor with shape {video_tensor.shape}\")\n",
    "    return video_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de6d3989-ae30-46fe-945a-5bba69f02c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "gloss_info = pd.read_csv('./gloss.csv')\n",
    "idx_to_word = {}\n",
    "word_to_idx = {}\n",
    "for i in range(len(gloss_info)):\n",
    "    idx_to_word[gloss_info['idx'][i]] = gloss_info['word'][i]\n",
    "    word_to_idx[gloss_info['word'][i]] = gloss_info['idx'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcd9f0db-9b0a-4a63-9978-8dcf3d120f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from model import SLR\n",
    "# # model = SLR(\n",
    "# #     n_embd=12*64, \n",
    "# #     n_cls_dict={'asl_citizen':2305, 'lsfb': 4657, 'wlasl':2000, 'autsl':226, 'rsl':1001},\n",
    "# #     n_head=12, \n",
    "# #     n_layer=4,\n",
    "# #     n_keypoints=63,\n",
    "# #     dropout=0.2, \n",
    "# #     max_len=64,\n",
    "# #     bias=True\n",
    "# # )\n",
    "\n",
    "# # model = torch.compile(model)\n",
    "# # model.load_state_dict(torch.load('./models/small_model.pth', map_location=torch.device('cpu')))\n",
    "\n",
    "\n",
    "# # Run a bigger model. About 2.5x larger. Validation accuracy is about the same however\n",
    "\n",
    "from model import SLR\n",
    "model = SLR(\n",
    "    n_embd=16*64, \n",
    "    n_cls_dict={'asl_citizen':2305, 'lsfb': 4657, 'wlasl':2000, 'autsl':226, 'rsl':1001},\n",
    "    n_head=16, \n",
    "    n_layer=6,\n",
    "    n_keypoints=63,\n",
    "    dropout=0.6, \n",
    "    max_len=64,\n",
    "    bias=True\n",
    ")\n",
    "\n",
    "model = torch.compile(model)\n",
    "model.load_state_dict(torch.load('./models/big_model.pth', map_location=torch.device('cpu')))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model.eval()\n",
    "# print(f'Trainable parameters: {model.num_params()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af570a46-0ca8-40b3-bd43-3d964d0bd364",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load a video or record it:\n",
    "\n",
    "#video = record_video_tensor(fps=20, record_time=3)\n",
    "video = read_video('./example3.mp4')\n",
    "video = video.permute(0, 3, 1, 2)/255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "196eeeaf-64bb-4765-a140-377df1b03f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from VideoLoader import KeypointExtractor\n",
    "# Over here it runs the media pipe model. Perhaps the biggest bottle neck overall. \n",
    "\n",
    "pose = KeypointExtractor().extract(video)\n",
    "height, width = video.shape[-2], video.shape[-1]\n",
    "del video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f602981-da8a-4be6-ae8e-d876fb9c2e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_keypoints = list(range(42)) \n",
    "selected_keypoints = selected_keypoints + [x + 42 for x in ([291, 267, 37, 61, 84, 314, 310, 13, 80, 14] + [152])]\n",
    "selected_keypoints = selected_keypoints + [x + 520 for x in ([2, 5, 7, 8, 11, 12, 13, 14, 15, 16])]\n",
    "\n",
    "\n",
    "flipped_selected_keypoints = list(range(21, 42)) + list(range(21)) \n",
    "flipped_selected_keypoints = flipped_selected_keypoints + [x + 42 for x in ([61, 37, 267, 291, 314, 84, 80, 13, 310, 14] + [152])]\n",
    "flipped_selected_keypoints = flipped_selected_keypoints + [x + 520 for x in ([5, 2, 8, 7, 12, 11, 14, 13, 16, 15])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "553e1219-75c2-4b49-bde6-bafe3501dbe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1610, 1584, 2148, 138, 927, 414, 1366, 483, 972, 772]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from VideoDataset import process_keypoints\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "# with augmentation to true, sample multiple frames and feed it to the model. take the average of the result.\n",
    "# Since torch.compile is used, the model is compiled the first time it is ran. Running it afterwards will be faster.\n",
    "\n",
    "sample_amount = 20 # Run the model 20 times\n",
    "\n",
    "logits = 0\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for i in range(sample_amount):\n",
    "        keypoints, valid_keypoints = process_keypoints(pose, 64, selected_keypoints, height=height, width=width, augment=True)\n",
    "        keypoints[:,:, 0] = keypoints[:,:, 0]\n",
    "        logits = logits + model.heads['asl_citizen'](model(keypoints.unsqueeze(0), valid_keypoints.unsqueeze(0)))\n",
    "\n",
    "idx = torch.argsort(logits, descending=True)[0].tolist()\n",
    "idx[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e198cda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\samty\\OneDrive\\Desktop\\Courses\\ecs193\\Sign-Language-App\\SignLanguageDemo\\VideoDataset.py:123: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  keypoints = torch.tensor(keypoints[indices])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1584"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_amount = 10\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    \n",
    "    # Simple approach - collect individual samples\n",
    "    keypoints_list = []\n",
    "    valid_keypoints_list = []\n",
    "    \n",
    "    for i in range(sample_amount):\n",
    "        # Get a single sample\n",
    "        single_keypoints, single_valid_keypoints = process_keypoints(\n",
    "            pose, 64, selected_keypoints, height=height, width=width, augment=True\n",
    "        )\n",
    "        \n",
    "        # Add to list (not using subscript operations)\n",
    "        keypoints_list.append(single_keypoints)\n",
    "        valid_keypoints_list.append(single_valid_keypoints)\n",
    "    \n",
    "    # Stack when done\n",
    "    keypoints_batch = torch.stack(keypoints_list)\n",
    "    valid_keypoints_batch = torch.stack(valid_keypoints_list)\n",
    "    \n",
    "    # Process batch\n",
    "    output_logits = model.heads['asl_citizen'](\n",
    "        model(keypoints_batch, valid_keypoints_batch)\n",
    "    )\n",
    "    \n",
    "    # Average logits\n",
    "    logits = output_logits.mean(dim=0)\n",
    "idx = torch.argsort(logits, descending=True)[0].tolist()\n",
    "idx   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c92937d-6ff5-4516-a91c-4d0e0c5e8d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 words\n",
      "library, snorkel, moon, cross, i love you\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 5 words\")\n",
    "print(', '.join([idx_to_word[idx[i]] for i in range(5)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f43f9b1-a45c-44a3-b2f3-c3f101b6b3bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1796"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx.index(word_to_idx['hug']) # search for a word's idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb25f3e-3535-42b7-8b1b-d333cca244d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
