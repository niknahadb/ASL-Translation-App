{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25a33497-3a4a-42d5-9f2b-4006deafbde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "GestureRecognizer = mp.tasks.vision.GestureRecognizer\n",
    "GestureRecognizerOptions = mp.tasks.vision.GestureRecognizerOptions\n",
    "GestureRecognizerResult = mp.tasks.vision.GestureRecognizerResult\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "\n",
    "options = GestureRecognizerOptions(\n",
    "    base_options=BaseOptions(model_asset_path='./gesture_recognizer.task'),\n",
    "    running_mode=VisionRunningMode.VIDEO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "900785c6-544b-4757-ae4b-6bae59c84ad3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m image\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Detections\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m results \u001b[38;5;241m=\u001b[39m recognizer\u001b[38;5;241m.\u001b[39mrecognize_for_video(\n\u001b[0;32m     19\u001b[0m     mp\u001b[38;5;241m.\u001b[39mImage(image_format\u001b[38;5;241m=\u001b[39mmp\u001b[38;5;241m.\u001b[39mImageFormat\u001b[38;5;241m.\u001b[39mSRGB, data\u001b[38;5;241m=\u001b[39mimage), \n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mint\u001b[39m((time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     24\u001b[0m image\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     25\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(image, cv2\u001b[38;5;241m.\u001b[39mCOLOR_RGB2BGR)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\mediapipe\\tasks\\python\\vision\\gesture_recognizer.py:420\u001b[0m, in \u001b[0;36mGestureRecognizer.recognize_for_video\u001b[1;34m(self, image, timestamp_ms, image_processing_options)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Performs gesture recognition on the provided video frame.\u001b[39;00m\n\u001b[0;32m    396\u001b[0m \n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03mOnly use this method when the GestureRecognizer is created with the video\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;124;03m  RuntimeError: If gesture recognition failed to run.\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    417\u001b[0m normalized_rect \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_to_normalized_rect(\n\u001b[0;32m    418\u001b[0m     image_processing_options, image, roi_allowed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    419\u001b[0m )\n\u001b[1;32m--> 420\u001b[0m output_packets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_video_data({\n\u001b[0;32m    421\u001b[0m     _IMAGE_IN_STREAM_NAME: packet_creator\u001b[38;5;241m.\u001b[39mcreate_image(image)\u001b[38;5;241m.\u001b[39mat(\n\u001b[0;32m    422\u001b[0m         timestamp_ms \u001b[38;5;241m*\u001b[39m _MICRO_SECONDS_PER_MILLISECOND\n\u001b[0;32m    423\u001b[0m     ),\n\u001b[0;32m    424\u001b[0m     _NORM_RECT_STREAM_NAME: packet_creator\u001b[38;5;241m.\u001b[39mcreate_proto(\n\u001b[0;32m    425\u001b[0m         normalized_rect\u001b[38;5;241m.\u001b[39mto_pb2()\n\u001b[0;32m    426\u001b[0m     )\u001b[38;5;241m.\u001b[39mat(timestamp_ms \u001b[38;5;241m*\u001b[39m _MICRO_SECONDS_PER_MILLISECOND),\n\u001b[0;32m    427\u001b[0m })\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_packets[_HAND_GESTURE_STREAM_NAME]\u001b[38;5;241m.\u001b[39mis_empty():\n\u001b[0;32m    430\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m GestureRecognizerResult([], [], [], [])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\mediapipe\\tasks\\python\\vision\\core\\base_vision_task_api.py:119\u001b[0m, in \u001b[0;36mBaseVisionTaskApi._process_video_data\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_running_mode \u001b[38;5;241m!=\u001b[39m _RunningMode\u001b[38;5;241m.\u001b[39mVIDEO:\n\u001b[0;32m    115\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    116\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTask is not initialized with the video mode. Current running mode:\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    117\u001b[0m       \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_running_mode\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m    118\u001b[0m   )\n\u001b[1;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_runner\u001b[38;5;241m.\u001b[39mprocess(inputs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with GestureRecognizer.create_from_options(options) as recognizer:\n",
    "\n",
    "    start_time = time.time()\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "    \n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        image = cv2.flip(image, 1)\n",
    "        image.flags.writeable = False\n",
    "        \n",
    "        # Detections\n",
    "        results = recognizer.recognize_for_video(\n",
    "            mp.Image(image_format=mp.ImageFormat.SRGB, data=image), \n",
    "            int((time.time() - start_time) * 1000)\n",
    "        )\n",
    "\n",
    "        \n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        if results.gestures and len(results.gestures) > 0:\n",
    "            detected_gesture = results.gestures[0][0].category_name  # Get the recognized gesture name\n",
    "\n",
    "            cv2.putText(image, f'Gesture: {detected_gesture}', (50, 50), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        \n",
    "        cv2.imshow('Hand Tracking', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3214e509-b73d-4fc6-ab26-50b214c2b427",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
