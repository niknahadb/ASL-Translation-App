{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "796a64ff-c209-48cd-8074-84d4018c5d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "from torchvision import transforms, models\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a801abdc-1184-4def-8ee9-419d773c4018",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AlphabetDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Root directory containing class folders with images.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.data = []\n",
    "        self.class_to_idx = {}\n",
    "\n",
    "        # Define image preprocessing (same as ImageNet normalization)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "        # Load all image paths and labels\n",
    "        for class_name in sorted(os.listdir(root_dir)):  # Ensure consistent class order\n",
    "            class_path = os.path.join(root_dir, class_name)\n",
    "            if os.path.isdir(class_path):  # Ensure it's a directory\n",
    "                if class_name not in self.class_to_idx:\n",
    "                    self.class_to_idx[class_name] = len(self.class_to_idx)\n",
    "                \n",
    "                for filename in os.listdir(class_path):\n",
    "                    file_path = os.path.join(class_path, filename)\n",
    "                    if file_path.endswith('.jpg') or file_path.endswith('.png'):  # Accept common image formats\n",
    "                        self.data.append((file_path, self.class_to_idx[class_name]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            image (torch.Tensor): Preprocessed image tensor.\n",
    "            label (int): Corresponding class index.\n",
    "        \"\"\"\n",
    "        file_path, label = self.data[idx]\n",
    "\n",
    "        # Load and transform the image\n",
    "        img = Image.open(file_path).convert(\"RGB\")  # Ensure it's RGB\n",
    "        img = self.transform(img)  # Apply transformations (no batch dimension)\n",
    "\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4d16d1af-6354-4c7c-9015-f517143880fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "root_directory = \"/workspace/dataset/asl_alphabet_train/asl_alphabet_train\"\n",
    "\n",
    "dataset = AlphabetDataset(root_directory)\n",
    "dataloader = DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=256, shuffle=False)  # No shuffling for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b16437d1-c1c5-407a-90f0-e14efa8eb143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "class ImageModel(nn.Module):\n",
    "    def __init__(self, n_class=28):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        efficientnet = timm.create_model('efficientnet_lite0', pretrained=True)\n",
    "        self.feature_extractor = torch.nn.Sequential(*list(efficientnet.children())[:-4])  # Remove classifier\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.feature_extractor.eval()\n",
    "\n",
    "        self.conv1x1 = nn.Conv2d(in_channels=320, out_channels=24, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        \n",
    "        self.linear = nn.Linear(1176, 128)\n",
    "        self.head = nn.Linear(128, n_class)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = self.feature_extractor(x)  # (B, 1280, 1, 1)\n",
    "\n",
    "        x = self.conv1x1(x)\n",
    "        x = self.relu(x)\n",
    "        x = x.reshape(-1, 1176)\n",
    "        x = self.relu(self.linear(x))  # (B, 128)\n",
    "        x = self.head(x)  # (B, n_class)\n",
    "        return x\n",
    "\n",
    "    def num_params(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8a226497-e284-4f14-9c36-310b61b135b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "ImageModel(\n",
      "  (feature_extractor): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNormAct2d(\n",
      "      32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "      (drop): Identity()\n",
      "      (act): ReLU6(inplace=True)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): DepthwiseSeparableConv(\n",
      "          (conv_dw): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): ReLU6(inplace=True)\n",
      "          )\n",
      "          (aa): Identity()\n",
      "          (se): Identity()\n",
      "          (conv_pw): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): InvertedResidual(\n",
      "          (conv_pw): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): ReLU6(inplace=True)\n",
      "          )\n",
      "          (conv_dw): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): ReLU6(inplace=True)\n",
      "          )\n",
      "          (aa): Identity()\n",
      "          (se): Identity()\n",
      "          (conv_pwl): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNormAct2d(\n",
      "            24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (1): InvertedResidual(\n",
      "          (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): ReLU6(inplace=True)\n",
      "          )\n",
      "          (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): ReLU6(inplace=True)\n",
      "          )\n",
      "          (aa): Identity()\n",
      "          (se): Identity()\n",
      "          (conv_pwl): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNormAct2d(\n",
      "            24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (0): InvertedResidual(\n",
      "          (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): ReLU6(inplace=True)\n",
      "          )\n",
      "          (conv_dw): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): ReLU6(inplace=True)\n",
      "          )\n",
      "          (aa): Identity()\n",
      "          (se): Identity()\n",
      "          (conv_pwl): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNormAct2d(\n",
      "            40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (1): InvertedResidual(\n",
      "          (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): ReLU6(inplace=True)\n",
      "          )\n",
      "          (conv_dw): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): ReLU6(inplace=True)\n",
      "          )\n",
      "          (aa): Identity()\n",
      "          (se): Identity()\n",
      "          (conv_pwl): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNormAct2d(\n",
      "            40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (0): InvertedResidual(\n",
      "          (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): ReLU6(inplace=True)\n",
      "          )\n",
      "          (conv_dw): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): ReLU6(inplace=True)\n",
      "          )\n",
      "          (aa): Identity()\n",
      "          (se): Identity()\n",
      "          (conv_pwl): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNormAct2d(\n",
      "            80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (1): InvertedResidual(\n",
      "          (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): ReLU6(inplace=True)\n",
      "          )\n",
      "          (conv_dw): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): ReLU6(inplace=True)\n",
      "          )\n",
      "          (aa): Identity()\n",
      "          (se): Identity()\n",
      "          (conv_pwl): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNormAct2d(\n",
      "            80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (2): InvertedResidual(\n",
      "          (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): ReLU6(inplace=True)\n",
      "          )\n",
      "          (conv_dw): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): ReLU6(inplace=True)\n",
      "          )\n",
      "          (aa): Identity()\n",
      "          (se): Identity()\n",
      "          (conv_pwl): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNormAct2d(\n",
      "            80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "      (4): Sequential(\n",
      "        (0): InvertedResidual(\n",
      "          (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): ReLU6(inplace=True)\n",
      "          )\n",
      "          (conv_dw): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): ReLU6(inplace=True)\n",
      "          )\n",
      "          (aa): Identity()\n",
      "          (se): Identity()\n",
      "          (conv_pwl): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNormAct2d(\n",
      "            112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (1): InvertedResidual(\n",
      "          (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): ReLU6(inplace=True)\n",
      "          )\n",
      "          (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): ReLU6(inplace=True)\n",
      "          )\n",
      "          (aa): Identity()\n",
      "          (se): Identity()\n",
      "          (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNormAct2d(\n",
      "            112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (2): InvertedResidual(\n",
      "          (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): ReLU6(inplace=True)\n",
      "          )\n",
      "          (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): ReLU6(inplace=True)\n",
      "          )\n",
      "          (aa): Identity()\n",
      "          (se): Identity()\n",
      "          (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNormAct2d(\n",
      "            112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "      (5): Sequential(\n",
      "        (0): InvertedResidual(\n",
      "          (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): ReLU6(inplace=True)\n",
      "          )\n",
      "          (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): ReLU6(inplace=True)\n",
      "          )\n",
      "          (aa): Identity()\n",
      "          (se): Identity()\n",
      "          (conv_pwl): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNormAct2d(\n",
      "            192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (1): InvertedResidual(\n",
      "          (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): ReLU6(inplace=True)\n",
      "          )\n",
      "          (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): ReLU6(inplace=True)\n",
      "          )\n",
      "          (aa): Identity()\n",
      "          (se): Identity()\n",
      "          (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNormAct2d(\n",
      "            192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (2): InvertedResidual(\n",
      "          (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): ReLU6(inplace=True)\n",
      "          )\n",
      "          (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): ReLU6(inplace=True)\n",
      "          )\n",
      "          (aa): Identity()\n",
      "          (se): Identity()\n",
      "          (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNormAct2d(\n",
      "            192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (3): InvertedResidual(\n",
      "          (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): ReLU6(inplace=True)\n",
      "          )\n",
      "          (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): ReLU6(inplace=True)\n",
      "          )\n",
      "          (aa): Identity()\n",
      "          (se): Identity()\n",
      "          (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNormAct2d(\n",
      "            192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "      (6): Sequential(\n",
      "        (0): InvertedResidual(\n",
      "          (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNormAct2d(\n",
      "            1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): ReLU6(inplace=True)\n",
      "          )\n",
      "          (conv_dw): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
      "          (bn2): BatchNormAct2d(\n",
      "            1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): ReLU6(inplace=True)\n",
      "          )\n",
      "          (aa): Identity()\n",
      "          (se): Identity()\n",
      "          (conv_pwl): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNormAct2d(\n",
      "            320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
      "            (drop): Identity()\n",
      "            (act): Identity()\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv1x1): Conv2d(320, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (linear): Linear(in_features=1176, out_features=128, bias=True)\n",
      "  (head): Linear(in_features=128, out_features=29, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "Number of parameters: 162077\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = ImageModel(\n",
    "    n_class = 29\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(device)\n",
    "print(model)\n",
    "print(f\"Number of parameters: {model.num_params()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7945f33e-de70-4a58-925d-ab98eb73233e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "epochs = 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc8c944-5c76-4fc7-8e62-57242cc211f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/500\n",
      "Batch 1/272 - Loss: 3.4071 - Accuracy: 3.12%\n",
      "Batch 2/272 - Loss: 3.3604 - Accuracy: 8.98%\n",
      "Batch 3/272 - Loss: 3.3480 - Accuracy: 4.30%\n",
      "Batch 4/272 - Loss: 3.2701 - Accuracy: 8.20%\n",
      "Batch 5/272 - Loss: 3.2427 - Accuracy: 11.33%\n",
      "Batch 6/272 - Loss: 3.1237 - Accuracy: 17.97%\n",
      "Batch 7/272 - Loss: 3.1023 - Accuracy: 14.84%\n",
      "Batch 8/272 - Loss: 3.0337 - Accuracy: 16.41%\n",
      "Batch 9/272 - Loss: 2.9498 - Accuracy: 20.31%\n",
      "Batch 10/272 - Loss: 2.9310 - Accuracy: 20.31%\n",
      "Batch 11/272 - Loss: 2.7912 - Accuracy: 30.08%\n",
      "Batch 12/272 - Loss: 2.6559 - Accuracy: 37.89%\n",
      "Batch 13/272 - Loss: 2.5506 - Accuracy: 37.89%\n",
      "Batch 14/272 - Loss: 2.5120 - Accuracy: 34.77%\n",
      "Batch 15/272 - Loss: 2.3924 - Accuracy: 39.45%\n",
      "Batch 16/272 - Loss: 2.3047 - Accuracy: 39.45%\n",
      "Batch 17/272 - Loss: 2.1494 - Accuracy: 44.53%\n",
      "Batch 18/272 - Loss: 2.1159 - Accuracy: 44.14%\n",
      "Batch 19/272 - Loss: 2.0062 - Accuracy: 49.61%\n",
      "Batch 20/272 - Loss: 1.9289 - Accuracy: 52.73%\n",
      "Batch 21/272 - Loss: 1.8183 - Accuracy: 51.17%\n",
      "Batch 22/272 - Loss: 1.7379 - Accuracy: 51.56%\n",
      "Batch 23/272 - Loss: 1.6410 - Accuracy: 53.91%\n",
      "Batch 24/272 - Loss: 1.5006 - Accuracy: 60.55%\n",
      "Batch 25/272 - Loss: 1.5083 - Accuracy: 58.59%\n",
      "Batch 26/272 - Loss: 1.3760 - Accuracy: 60.94%\n",
      "Batch 27/272 - Loss: 1.3808 - Accuracy: 58.59%\n",
      "Batch 28/272 - Loss: 1.3748 - Accuracy: 60.16%\n",
      "Batch 29/272 - Loss: 1.2919 - Accuracy: 64.84%\n",
      "Batch 30/272 - Loss: 1.1079 - Accuracy: 66.80%\n",
      "Batch 31/272 - Loss: 1.1191 - Accuracy: 69.92%\n",
      "Batch 32/272 - Loss: 1.1047 - Accuracy: 67.58%\n",
      "Batch 33/272 - Loss: 0.9187 - Accuracy: 71.48%\n",
      "Batch 34/272 - Loss: 0.9758 - Accuracy: 73.83%\n",
      "Batch 35/272 - Loss: 0.9284 - Accuracy: 74.22%\n",
      "Batch 36/272 - Loss: 0.9467 - Accuracy: 71.48%\n",
      "Batch 37/272 - Loss: 1.0249 - Accuracy: 65.23%\n",
      "Batch 38/272 - Loss: 0.7902 - Accuracy: 75.78%\n",
      "Batch 39/272 - Loss: 0.7624 - Accuracy: 76.95%\n",
      "Batch 40/272 - Loss: 0.7334 - Accuracy: 78.12%\n",
      "Batch 41/272 - Loss: 0.8089 - Accuracy: 76.17%\n",
      "Batch 42/272 - Loss: 0.7903 - Accuracy: 76.95%\n",
      "Batch 43/272 - Loss: 0.7569 - Accuracy: 75.78%\n",
      "Batch 44/272 - Loss: 0.7347 - Accuracy: 76.56%\n",
      "Batch 45/272 - Loss: 0.7751 - Accuracy: 76.95%\n",
      "Batch 46/272 - Loss: 0.6960 - Accuracy: 78.52%\n",
      "Batch 47/272 - Loss: 0.5804 - Accuracy: 80.86%\n",
      "Batch 48/272 - Loss: 0.6092 - Accuracy: 81.25%\n",
      "Batch 49/272 - Loss: 0.6280 - Accuracy: 81.25%\n",
      "Batch 50/272 - Loss: 0.6829 - Accuracy: 75.00%\n",
      "Batch 51/272 - Loss: 0.5582 - Accuracy: 82.03%\n",
      "Batch 52/272 - Loss: 0.5223 - Accuracy: 82.03%\n",
      "Batch 53/272 - Loss: 0.5106 - Accuracy: 82.42%\n",
      "Batch 54/272 - Loss: 0.6103 - Accuracy: 82.81%\n",
      "Batch 55/272 - Loss: 0.5060 - Accuracy: 82.42%\n",
      "Batch 56/272 - Loss: 0.5066 - Accuracy: 82.81%\n",
      "Batch 57/272 - Loss: 0.4946 - Accuracy: 82.42%\n",
      "Batch 58/272 - Loss: 0.5649 - Accuracy: 80.86%\n",
      "Batch 59/272 - Loss: 0.4946 - Accuracy: 81.25%\n",
      "Batch 60/272 - Loss: 0.5625 - Accuracy: 82.03%\n",
      "Batch 61/272 - Loss: 0.4304 - Accuracy: 85.16%\n",
      "Batch 62/272 - Loss: 0.3881 - Accuracy: 88.28%\n",
      "Batch 63/272 - Loss: 0.4627 - Accuracy: 85.94%\n",
      "Batch 64/272 - Loss: 0.4572 - Accuracy: 85.16%\n",
      "Batch 65/272 - Loss: 0.5143 - Accuracy: 83.98%\n",
      "Batch 66/272 - Loss: 0.4774 - Accuracy: 84.77%\n",
      "Batch 67/272 - Loss: 0.4391 - Accuracy: 84.77%\n",
      "Batch 68/272 - Loss: 0.4577 - Accuracy: 83.98%\n",
      "Batch 69/272 - Loss: 0.3546 - Accuracy: 86.72%\n",
      "Batch 70/272 - Loss: 0.4674 - Accuracy: 85.94%\n",
      "Batch 71/272 - Loss: 0.3826 - Accuracy: 86.72%\n",
      "Batch 72/272 - Loss: 0.4558 - Accuracy: 84.38%\n",
      "Batch 73/272 - Loss: 0.4322 - Accuracy: 84.38%\n",
      "Batch 74/272 - Loss: 0.3535 - Accuracy: 86.33%\n",
      "Batch 75/272 - Loss: 0.4046 - Accuracy: 87.11%\n",
      "Batch 76/272 - Loss: 0.3460 - Accuracy: 87.11%\n",
      "Batch 77/272 - Loss: 0.4895 - Accuracy: 83.98%\n",
      "Batch 78/272 - Loss: 0.3675 - Accuracy: 87.50%\n",
      "Batch 79/272 - Loss: 0.3570 - Accuracy: 85.94%\n",
      "Batch 80/272 - Loss: 0.2951 - Accuracy: 89.84%\n",
      "Batch 81/272 - Loss: 0.3406 - Accuracy: 88.28%\n",
      "Batch 82/272 - Loss: 0.3791 - Accuracy: 88.28%\n",
      "Batch 83/272 - Loss: 0.3787 - Accuracy: 89.45%\n",
      "Batch 84/272 - Loss: 0.3915 - Accuracy: 85.55%\n",
      "Batch 85/272 - Loss: 0.3524 - Accuracy: 86.72%\n",
      "Batch 86/272 - Loss: 0.3337 - Accuracy: 87.50%\n",
      "Batch 87/272 - Loss: 0.3176 - Accuracy: 90.62%\n",
      "Batch 88/272 - Loss: 0.4477 - Accuracy: 86.72%\n",
      "Batch 89/272 - Loss: 0.3925 - Accuracy: 85.55%\n",
      "Batch 90/272 - Loss: 0.3483 - Accuracy: 88.28%\n",
      "Batch 91/272 - Loss: 0.2605 - Accuracy: 91.02%\n",
      "Batch 92/272 - Loss: 0.4086 - Accuracy: 85.55%\n",
      "Batch 93/272 - Loss: 0.4036 - Accuracy: 85.16%\n",
      "Batch 94/272 - Loss: 0.3420 - Accuracy: 88.67%\n",
      "Batch 95/272 - Loss: 0.2690 - Accuracy: 91.41%\n",
      "Batch 96/272 - Loss: 0.3645 - Accuracy: 87.11%\n",
      "Batch 97/272 - Loss: 0.3393 - Accuracy: 87.11%\n",
      "Batch 98/272 - Loss: 0.3847 - Accuracy: 89.06%\n",
      "Batch 99/272 - Loss: 0.3846 - Accuracy: 86.72%\n",
      "Batch 100/272 - Loss: 0.2664 - Accuracy: 91.41%\n",
      "Batch 101/272 - Loss: 0.3807 - Accuracy: 86.72%\n",
      "Batch 102/272 - Loss: 0.3381 - Accuracy: 87.50%\n",
      "Batch 103/272 - Loss: 0.3788 - Accuracy: 89.45%\n",
      "Batch 104/272 - Loss: 0.2971 - Accuracy: 90.62%\n",
      "Batch 105/272 - Loss: 0.3188 - Accuracy: 89.84%\n",
      "Batch 106/272 - Loss: 0.2984 - Accuracy: 91.41%\n",
      "Batch 107/272 - Loss: 0.2320 - Accuracy: 94.14%\n",
      "Batch 108/272 - Loss: 0.4312 - Accuracy: 87.11%\n",
      "Batch 109/272 - Loss: 0.2930 - Accuracy: 89.84%\n",
      "Batch 110/272 - Loss: 0.3255 - Accuracy: 90.23%\n",
      "Batch 111/272 - Loss: 0.2887 - Accuracy: 90.23%\n",
      "Batch 112/272 - Loss: 0.2764 - Accuracy: 92.19%\n",
      "Batch 113/272 - Loss: 0.3214 - Accuracy: 89.06%\n",
      "Batch 114/272 - Loss: 0.1951 - Accuracy: 94.92%\n",
      "Batch 115/272 - Loss: 0.2371 - Accuracy: 93.36%\n",
      "Batch 116/272 - Loss: 0.3764 - Accuracy: 85.55%\n",
      "Batch 117/272 - Loss: 0.3845 - Accuracy: 88.28%\n",
      "Batch 118/272 - Loss: 0.2797 - Accuracy: 91.02%\n",
      "Batch 119/272 - Loss: 0.3566 - Accuracy: 86.72%\n",
      "Batch 120/272 - Loss: 0.2237 - Accuracy: 92.19%\n",
      "Batch 121/272 - Loss: 0.2103 - Accuracy: 93.75%\n",
      "Batch 122/272 - Loss: 0.2220 - Accuracy: 92.97%\n",
      "Batch 123/272 - Loss: 0.3174 - Accuracy: 86.72%\n",
      "Batch 124/272 - Loss: 0.3004 - Accuracy: 91.80%\n",
      "Batch 125/272 - Loss: 0.2913 - Accuracy: 91.02%\n",
      "Batch 126/272 - Loss: 0.2471 - Accuracy: 91.41%\n",
      "Batch 127/272 - Loss: 0.2980 - Accuracy: 88.67%\n",
      "Batch 128/272 - Loss: 0.2805 - Accuracy: 89.84%\n",
      "Batch 129/272 - Loss: 0.2333 - Accuracy: 91.80%\n",
      "Batch 130/272 - Loss: 0.2328 - Accuracy: 92.58%\n",
      "Batch 131/272 - Loss: 0.2598 - Accuracy: 92.19%\n",
      "Batch 132/272 - Loss: 0.2313 - Accuracy: 91.80%\n",
      "Batch 133/272 - Loss: 0.2977 - Accuracy: 89.84%\n",
      "Batch 134/272 - Loss: 0.2303 - Accuracy: 92.19%\n",
      "Batch 135/272 - Loss: 0.2320 - Accuracy: 92.19%\n",
      "Batch 136/272 - Loss: 0.2114 - Accuracy: 92.19%\n",
      "Batch 137/272 - Loss: 0.3397 - Accuracy: 88.67%\n",
      "Batch 138/272 - Loss: 0.2091 - Accuracy: 92.19%\n",
      "Batch 139/272 - Loss: 0.2645 - Accuracy: 90.62%\n",
      "Batch 140/272 - Loss: 0.2181 - Accuracy: 91.41%\n",
      "Batch 141/272 - Loss: 0.2180 - Accuracy: 92.19%\n",
      "Batch 142/272 - Loss: 0.2391 - Accuracy: 93.36%\n",
      "Batch 143/272 - Loss: 0.2030 - Accuracy: 92.19%\n",
      "Batch 144/272 - Loss: 0.2372 - Accuracy: 92.19%\n",
      "Batch 145/272 - Loss: 0.2679 - Accuracy: 91.41%\n",
      "Batch 146/272 - Loss: 0.2400 - Accuracy: 92.19%\n",
      "Batch 147/272 - Loss: 0.2091 - Accuracy: 92.58%\n",
      "Batch 148/272 - Loss: 0.2289 - Accuracy: 92.19%\n",
      "Batch 149/272 - Loss: 0.1699 - Accuracy: 94.92%\n",
      "Batch 150/272 - Loss: 0.2203 - Accuracy: 94.14%\n",
      "Batch 151/272 - Loss: 0.1672 - Accuracy: 95.31%\n",
      "Batch 152/272 - Loss: 0.2079 - Accuracy: 92.58%\n",
      "Batch 153/272 - Loss: 0.2367 - Accuracy: 92.97%\n",
      "Batch 154/272 - Loss: 0.1679 - Accuracy: 95.70%\n",
      "Batch 155/272 - Loss: 0.1830 - Accuracy: 93.36%\n",
      "Batch 156/272 - Loss: 0.2074 - Accuracy: 93.75%\n",
      "Batch 157/272 - Loss: 0.1665 - Accuracy: 94.53%\n",
      "Batch 158/272 - Loss: 0.2353 - Accuracy: 91.41%\n",
      "Batch 159/272 - Loss: 0.1679 - Accuracy: 92.97%\n",
      "Batch 160/272 - Loss: 0.3241 - Accuracy: 88.67%\n",
      "Batch 161/272 - Loss: 0.1448 - Accuracy: 93.75%\n",
      "Batch 162/272 - Loss: 0.2139 - Accuracy: 92.19%\n",
      "Batch 163/272 - Loss: 0.1742 - Accuracy: 94.14%\n",
      "Batch 164/272 - Loss: 0.1372 - Accuracy: 96.09%\n",
      "Batch 165/272 - Loss: 0.2061 - Accuracy: 92.19%\n",
      "Batch 166/272 - Loss: 0.1628 - Accuracy: 94.92%\n",
      "Batch 167/272 - Loss: 0.2153 - Accuracy: 92.19%\n",
      "Batch 168/272 - Loss: 0.1388 - Accuracy: 95.70%\n",
      "Batch 169/272 - Loss: 0.1287 - Accuracy: 95.31%\n",
      "Batch 170/272 - Loss: 0.2669 - Accuracy: 91.41%\n",
      "Batch 171/272 - Loss: 0.1549 - Accuracy: 94.53%\n",
      "Batch 172/272 - Loss: 0.2473 - Accuracy: 93.36%\n",
      "Batch 173/272 - Loss: 0.1528 - Accuracy: 94.53%\n",
      "Batch 174/272 - Loss: 0.2423 - Accuracy: 90.62%\n",
      "Batch 175/272 - Loss: 0.1972 - Accuracy: 93.75%\n",
      "Batch 176/272 - Loss: 0.1774 - Accuracy: 94.53%\n",
      "Batch 177/272 - Loss: 0.1319 - Accuracy: 95.70%\n",
      "Batch 178/272 - Loss: 0.2158 - Accuracy: 94.53%\n",
      "Batch 179/272 - Loss: 0.2093 - Accuracy: 92.97%\n",
      "Batch 180/272 - Loss: 0.1670 - Accuracy: 93.75%\n",
      "Batch 181/272 - Loss: 0.1676 - Accuracy: 94.92%\n",
      "Batch 182/272 - Loss: 0.2588 - Accuracy: 90.23%\n",
      "Batch 183/272 - Loss: 0.1485 - Accuracy: 94.14%\n",
      "Batch 184/272 - Loss: 0.1296 - Accuracy: 96.48%\n",
      "Batch 185/272 - Loss: 0.2646 - Accuracy: 91.80%\n",
      "Batch 186/272 - Loss: 0.1914 - Accuracy: 93.75%\n",
      "Batch 187/272 - Loss: 0.1489 - Accuracy: 94.53%\n",
      "Batch 188/272 - Loss: 0.1439 - Accuracy: 96.09%\n",
      "Batch 189/272 - Loss: 0.1291 - Accuracy: 95.70%\n",
      "Batch 190/272 - Loss: 0.1873 - Accuracy: 92.97%\n",
      "Batch 191/272 - Loss: 0.1461 - Accuracy: 95.31%\n",
      "Batch 192/272 - Loss: 0.1624 - Accuracy: 94.14%\n",
      "Batch 193/272 - Loss: 0.1553 - Accuracy: 94.53%\n",
      "Batch 194/272 - Loss: 0.1846 - Accuracy: 95.31%\n",
      "Batch 195/272 - Loss: 0.1748 - Accuracy: 94.14%\n",
      "Batch 196/272 - Loss: 0.1670 - Accuracy: 94.92%\n",
      "Batch 197/272 - Loss: 0.1654 - Accuracy: 94.53%\n",
      "Batch 198/272 - Loss: 0.2095 - Accuracy: 92.58%\n",
      "Batch 199/272 - Loss: 0.1335 - Accuracy: 96.48%\n",
      "Batch 200/272 - Loss: 0.1682 - Accuracy: 94.53%\n",
      "Batch 201/272 - Loss: 0.1594 - Accuracy: 94.53%\n",
      "Batch 202/272 - Loss: 0.1324 - Accuracy: 96.48%\n",
      "Batch 203/272 - Loss: 0.1696 - Accuracy: 94.92%\n",
      "Batch 204/272 - Loss: 0.3050 - Accuracy: 91.02%\n",
      "Batch 205/272 - Loss: 0.1480 - Accuracy: 94.92%\n",
      "Batch 206/272 - Loss: 0.1781 - Accuracy: 92.58%\n",
      "Batch 207/272 - Loss: 0.1559 - Accuracy: 94.92%\n",
      "Batch 208/272 - Loss: 0.2211 - Accuracy: 93.75%\n",
      "Batch 209/272 - Loss: 0.1809 - Accuracy: 93.36%\n",
      "Batch 210/272 - Loss: 0.1837 - Accuracy: 93.75%\n",
      "Batch 211/272 - Loss: 0.1678 - Accuracy: 95.31%\n",
      "Batch 212/272 - Loss: 0.1700 - Accuracy: 95.31%\n",
      "Batch 213/272 - Loss: 0.1983 - Accuracy: 93.75%\n",
      "Batch 214/272 - Loss: 0.1283 - Accuracy: 95.70%\n",
      "Batch 215/272 - Loss: 0.1782 - Accuracy: 92.97%\n",
      "Batch 216/272 - Loss: 0.1300 - Accuracy: 95.31%\n",
      "Batch 217/272 - Loss: 0.1332 - Accuracy: 96.88%\n",
      "Batch 218/272 - Loss: 0.1211 - Accuracy: 96.88%\n",
      "Batch 219/272 - Loss: 0.1628 - Accuracy: 95.70%\n",
      "Batch 220/272 - Loss: 0.1673 - Accuracy: 93.75%\n",
      "Batch 221/272 - Loss: 0.1331 - Accuracy: 94.53%\n",
      "Batch 222/272 - Loss: 0.1829 - Accuracy: 95.70%\n",
      "Batch 223/272 - Loss: 0.1678 - Accuracy: 93.75%\n",
      "Batch 224/272 - Loss: 0.1525 - Accuracy: 94.14%\n",
      "Batch 225/272 - Loss: 0.1947 - Accuracy: 94.53%\n",
      "Batch 226/272 - Loss: 0.1217 - Accuracy: 96.48%\n",
      "Batch 227/272 - Loss: 0.1508 - Accuracy: 93.75%\n",
      "Batch 228/272 - Loss: 0.1456 - Accuracy: 93.75%\n",
      "Batch 229/272 - Loss: 0.1388 - Accuracy: 96.09%\n",
      "Batch 230/272 - Loss: 0.1195 - Accuracy: 96.88%\n",
      "Batch 231/272 - Loss: 0.1191 - Accuracy: 96.48%\n",
      "Batch 232/272 - Loss: 0.1361 - Accuracy: 96.09%\n",
      "Batch 233/272 - Loss: 0.1165 - Accuracy: 96.09%\n",
      "Batch 234/272 - Loss: 0.1477 - Accuracy: 93.75%\n",
      "Batch 235/272 - Loss: 0.1272 - Accuracy: 96.09%\n",
      "Batch 236/272 - Loss: 0.1533 - Accuracy: 94.92%\n",
      "Batch 237/272 - Loss: 0.1692 - Accuracy: 94.14%\n",
      "Batch 238/272 - Loss: 0.1581 - Accuracy: 94.14%\n",
      "Batch 239/272 - Loss: 0.1780 - Accuracy: 95.31%\n",
      "Batch 240/272 - Loss: 0.1843 - Accuracy: 92.58%\n",
      "Batch 241/272 - Loss: 0.1399 - Accuracy: 94.53%\n",
      "Batch 242/272 - Loss: 0.1924 - Accuracy: 92.97%\n",
      "Batch 243/272 - Loss: 0.1770 - Accuracy: 94.14%\n",
      "Batch 244/272 - Loss: 0.1197 - Accuracy: 95.70%\n",
      "Batch 245/272 - Loss: 0.1558 - Accuracy: 94.14%\n",
      "Batch 246/272 - Loss: 0.1480 - Accuracy: 95.70%\n",
      "Batch 247/272 - Loss: 0.1252 - Accuracy: 94.92%\n",
      "Batch 248/272 - Loss: 0.0883 - Accuracy: 97.27%\n",
      "Batch 249/272 - Loss: 0.1508 - Accuracy: 96.09%\n",
      "Batch 250/272 - Loss: 0.1837 - Accuracy: 92.97%\n",
      "Batch 251/272 - Loss: 0.1862 - Accuracy: 93.36%\n",
      "Batch 252/272 - Loss: 0.1749 - Accuracy: 94.14%\n",
      "Batch 253/272 - Loss: 0.1328 - Accuracy: 96.88%\n",
      "Batch 254/272 - Loss: 0.1218 - Accuracy: 96.09%\n",
      "Batch 255/272 - Loss: 0.1173 - Accuracy: 95.70%\n",
      "Batch 256/272 - Loss: 0.1601 - Accuracy: 94.53%\n",
      "Batch 257/272 - Loss: 0.1100 - Accuracy: 95.70%\n",
      "Batch 258/272 - Loss: 0.1430 - Accuracy: 95.70%\n",
      "Batch 259/272 - Loss: 0.1534 - Accuracy: 95.31%\n",
      "Batch 260/272 - Loss: 0.1643 - Accuracy: 93.75%\n",
      "Batch 261/272 - Loss: 0.0999 - Accuracy: 96.88%\n",
      "Batch 262/272 - Loss: 0.0990 - Accuracy: 97.66%\n",
      "Batch 263/272 - Loss: 0.1101 - Accuracy: 94.92%\n",
      "Batch 264/272 - Loss: 0.1592 - Accuracy: 94.14%\n",
      "Batch 265/272 - Loss: 0.1261 - Accuracy: 95.70%\n",
      "Batch 266/272 - Loss: 0.1384 - Accuracy: 94.92%\n",
      "Batch 267/272 - Loss: 0.1308 - Accuracy: 96.09%\n",
      "Batch 268/272 - Loss: 0.1654 - Accuracy: 92.19%\n",
      "Batch 269/272 - Loss: 0.1078 - Accuracy: 94.92%\n",
      "Batch 270/272 - Loss: 0.1521 - Accuracy: 94.92%\n",
      "Batch 271/272 - Loss: 0.1408 - Accuracy: 94.14%\n",
      "Batch 272/272 - Loss: 0.1662 - Accuracy: 93.30%\n",
      "Training - Avg Loss: 0.5175, Accuracy: 84.57%\n",
      "Validation - Avg Loss: 0.1229, Accuracy: 95.95%\n",
      "\n",
      "\n",
      "Epoch 2/500\n",
      "Batch 1/272 - Loss: 0.0873 - Accuracy: 98.05%\n",
      "Batch 2/272 - Loss: 0.0539 - Accuracy: 98.83%\n",
      "Batch 3/272 - Loss: 0.1280 - Accuracy: 96.88%\n",
      "Batch 4/272 - Loss: 0.0795 - Accuracy: 97.66%\n",
      "Batch 5/272 - Loss: 0.1023 - Accuracy: 96.88%\n",
      "Batch 6/272 - Loss: 0.0922 - Accuracy: 97.66%\n",
      "Batch 7/272 - Loss: 0.1021 - Accuracy: 96.09%\n",
      "Batch 8/272 - Loss: 0.0947 - Accuracy: 96.88%\n",
      "Batch 9/272 - Loss: 0.1099 - Accuracy: 97.27%\n",
      "Batch 10/272 - Loss: 0.1506 - Accuracy: 94.14%\n",
      "Batch 11/272 - Loss: 0.1040 - Accuracy: 97.27%\n",
      "Batch 12/272 - Loss: 0.0586 - Accuracy: 98.05%\n",
      "Batch 13/272 - Loss: 0.0904 - Accuracy: 98.83%\n",
      "Batch 14/272 - Loss: 0.1485 - Accuracy: 95.70%\n",
      "Batch 15/272 - Loss: 0.0942 - Accuracy: 96.09%\n",
      "Batch 16/272 - Loss: 0.1016 - Accuracy: 96.09%\n",
      "Batch 17/272 - Loss: 0.0959 - Accuracy: 96.48%\n",
      "Batch 18/272 - Loss: 0.1058 - Accuracy: 95.31%\n",
      "Batch 19/272 - Loss: 0.1251 - Accuracy: 95.70%\n",
      "Batch 20/272 - Loss: 0.0838 - Accuracy: 97.27%\n",
      "Batch 21/272 - Loss: 0.1037 - Accuracy: 96.88%\n",
      "Batch 22/272 - Loss: 0.1111 - Accuracy: 96.88%\n",
      "Batch 23/272 - Loss: 0.1062 - Accuracy: 95.31%\n",
      "Batch 24/272 - Loss: 0.0694 - Accuracy: 98.83%\n",
      "Batch 25/272 - Loss: 0.0924 - Accuracy: 97.66%\n",
      "Batch 26/272 - Loss: 0.0671 - Accuracy: 97.66%\n",
      "Batch 27/272 - Loss: 0.1071 - Accuracy: 96.48%\n",
      "Batch 28/272 - Loss: 0.1187 - Accuracy: 96.48%\n",
      "Batch 29/272 - Loss: 0.1166 - Accuracy: 95.70%\n",
      "Batch 30/272 - Loss: 0.0715 - Accuracy: 98.05%\n",
      "Batch 31/272 - Loss: 0.0956 - Accuracy: 97.66%\n",
      "Batch 32/272 - Loss: 0.1223 - Accuracy: 95.31%\n",
      "Batch 33/272 - Loss: 0.1405 - Accuracy: 94.14%\n",
      "Batch 34/272 - Loss: 0.1261 - Accuracy: 95.31%\n",
      "Batch 35/272 - Loss: 0.0890 - Accuracy: 97.66%\n",
      "Batch 36/272 - Loss: 0.0672 - Accuracy: 98.44%\n",
      "Batch 37/272 - Loss: 0.0959 - Accuracy: 97.66%\n",
      "Batch 38/272 - Loss: 0.0825 - Accuracy: 97.66%\n",
      "Batch 39/272 - Loss: 0.0823 - Accuracy: 97.27%\n",
      "Batch 40/272 - Loss: 0.1004 - Accuracy: 95.70%\n",
      "Batch 41/272 - Loss: 0.0825 - Accuracy: 98.05%\n",
      "Batch 42/272 - Loss: 0.0916 - Accuracy: 96.09%\n",
      "Batch 43/272 - Loss: 0.0856 - Accuracy: 97.66%\n",
      "Batch 44/272 - Loss: 0.0973 - Accuracy: 96.48%\n",
      "Batch 45/272 - Loss: 0.1026 - Accuracy: 96.48%\n",
      "Batch 46/272 - Loss: 0.0972 - Accuracy: 97.27%\n",
      "Batch 47/272 - Loss: 0.0758 - Accuracy: 98.44%\n",
      "Batch 48/272 - Loss: 0.1029 - Accuracy: 96.88%\n",
      "Batch 49/272 - Loss: 0.0954 - Accuracy: 97.27%\n",
      "Batch 50/272 - Loss: 0.1316 - Accuracy: 95.31%\n",
      "Batch 51/272 - Loss: 0.1408 - Accuracy: 96.09%\n",
      "Batch 52/272 - Loss: 0.0952 - Accuracy: 96.48%\n",
      "Batch 53/272 - Loss: 0.0902 - Accuracy: 97.27%\n",
      "Batch 54/272 - Loss: 0.0724 - Accuracy: 96.88%\n",
      "Batch 55/272 - Loss: 0.1031 - Accuracy: 95.70%\n",
      "Batch 56/272 - Loss: 0.0641 - Accuracy: 97.27%\n",
      "Batch 57/272 - Loss: 0.0531 - Accuracy: 98.83%\n",
      "Batch 58/272 - Loss: 0.1096 - Accuracy: 97.27%\n",
      "Batch 59/272 - Loss: 0.0614 - Accuracy: 98.44%\n",
      "Batch 60/272 - Loss: 0.0775 - Accuracy: 97.27%\n",
      "Batch 61/272 - Loss: 0.0964 - Accuracy: 96.88%\n",
      "Batch 62/272 - Loss: 0.1126 - Accuracy: 95.70%\n",
      "Batch 63/272 - Loss: 0.0847 - Accuracy: 96.48%\n",
      "Batch 64/272 - Loss: 0.0890 - Accuracy: 97.27%\n",
      "Batch 65/272 - Loss: 0.0834 - Accuracy: 96.88%\n",
      "Batch 66/272 - Loss: 0.0853 - Accuracy: 97.27%\n",
      "Batch 67/272 - Loss: 0.0694 - Accuracy: 98.05%\n",
      "Batch 68/272 - Loss: 0.0943 - Accuracy: 96.88%\n",
      "Batch 69/272 - Loss: 0.0578 - Accuracy: 98.44%\n",
      "Batch 70/272 - Loss: 0.1081 - Accuracy: 95.70%\n",
      "Batch 71/272 - Loss: 0.0699 - Accuracy: 97.27%\n",
      "Batch 72/272 - Loss: 0.1060 - Accuracy: 94.92%\n",
      "Batch 73/272 - Loss: 0.0743 - Accuracy: 97.27%\n",
      "Batch 74/272 - Loss: 0.0886 - Accuracy: 97.27%\n",
      "Batch 75/272 - Loss: 0.0904 - Accuracy: 97.27%\n",
      "Batch 76/272 - Loss: 0.0554 - Accuracy: 98.44%\n",
      "Batch 77/272 - Loss: 0.0877 - Accuracy: 97.27%\n",
      "Batch 78/272 - Loss: 0.0588 - Accuracy: 98.44%\n",
      "Batch 79/272 - Loss: 0.0756 - Accuracy: 96.48%\n",
      "Batch 80/272 - Loss: 0.1069 - Accuracy: 95.70%\n",
      "Batch 81/272 - Loss: 0.1092 - Accuracy: 95.70%\n",
      "Batch 82/272 - Loss: 0.1073 - Accuracy: 96.09%\n",
      "Batch 83/272 - Loss: 0.1264 - Accuracy: 94.92%\n",
      "Batch 84/272 - Loss: 0.0760 - Accuracy: 97.27%\n",
      "Batch 85/272 - Loss: 0.1213 - Accuracy: 96.48%\n",
      "Batch 86/272 - Loss: 0.0520 - Accuracy: 98.83%\n",
      "Batch 87/272 - Loss: 0.0393 - Accuracy: 99.22%\n",
      "Batch 88/272 - Loss: 0.0622 - Accuracy: 98.83%\n",
      "Batch 89/272 - Loss: 0.1333 - Accuracy: 96.88%\n",
      "Batch 90/272 - Loss: 0.1266 - Accuracy: 96.48%\n",
      "Batch 91/272 - Loss: 0.0760 - Accuracy: 96.88%\n",
      "Batch 92/272 - Loss: 0.0650 - Accuracy: 98.44%\n",
      "Batch 93/272 - Loss: 0.0522 - Accuracy: 98.83%\n",
      "Batch 94/272 - Loss: 0.1107 - Accuracy: 96.09%\n",
      "Batch 95/272 - Loss: 0.1141 - Accuracy: 96.88%\n",
      "Batch 96/272 - Loss: 0.0705 - Accuracy: 96.88%\n",
      "Batch 97/272 - Loss: 0.0767 - Accuracy: 97.66%\n",
      "Batch 98/272 - Loss: 0.0567 - Accuracy: 98.83%\n",
      "Batch 99/272 - Loss: 0.1210 - Accuracy: 94.92%\n",
      "Batch 100/272 - Loss: 0.1000 - Accuracy: 97.66%\n",
      "Batch 101/272 - Loss: 0.0720 - Accuracy: 97.27%\n",
      "Batch 102/272 - Loss: 0.0848 - Accuracy: 97.27%\n",
      "Batch 103/272 - Loss: 0.0708 - Accuracy: 98.44%\n",
      "Batch 104/272 - Loss: 0.0572 - Accuracy: 98.05%\n",
      "Batch 105/272 - Loss: 0.0593 - Accuracy: 98.05%\n",
      "Batch 106/272 - Loss: 0.0917 - Accuracy: 97.66%\n",
      "Batch 107/272 - Loss: 0.1129 - Accuracy: 96.88%\n",
      "Batch 108/272 - Loss: 0.1050 - Accuracy: 95.70%\n",
      "Batch 109/272 - Loss: 0.0785 - Accuracy: 97.27%\n",
      "Batch 110/272 - Loss: 0.0811 - Accuracy: 96.09%\n",
      "Batch 111/272 - Loss: 0.0515 - Accuracy: 98.05%\n",
      "Batch 112/272 - Loss: 0.0881 - Accuracy: 96.88%\n",
      "Batch 113/272 - Loss: 0.1747 - Accuracy: 94.14%\n",
      "Batch 114/272 - Loss: 0.0885 - Accuracy: 96.09%\n",
      "Batch 115/272 - Loss: 0.0642 - Accuracy: 97.27%\n",
      "Batch 116/272 - Loss: 0.0835 - Accuracy: 97.27%\n",
      "Batch 117/272 - Loss: 0.0737 - Accuracy: 97.27%\n",
      "Batch 118/272 - Loss: 0.0923 - Accuracy: 96.48%\n",
      "Batch 119/272 - Loss: 0.1026 - Accuracy: 96.48%\n",
      "Batch 120/272 - Loss: 0.1200 - Accuracy: 96.88%\n",
      "Batch 121/272 - Loss: 0.0737 - Accuracy: 96.88%\n",
      "Batch 122/272 - Loss: 0.0941 - Accuracy: 96.88%\n",
      "Batch 123/272 - Loss: 0.0659 - Accuracy: 97.27%\n",
      "Batch 124/272 - Loss: 0.1018 - Accuracy: 96.88%\n",
      "Batch 125/272 - Loss: 0.1066 - Accuracy: 96.09%\n",
      "Batch 126/272 - Loss: 0.1097 - Accuracy: 96.48%\n",
      "Batch 127/272 - Loss: 0.0798 - Accuracy: 97.27%\n",
      "Batch 128/272 - Loss: 0.0836 - Accuracy: 97.66%\n",
      "Batch 129/272 - Loss: 0.0706 - Accuracy: 98.44%\n",
      "Batch 130/272 - Loss: 0.0834 - Accuracy: 98.05%\n",
      "Batch 131/272 - Loss: 0.1009 - Accuracy: 97.66%\n",
      "Batch 132/272 - Loss: 0.1237 - Accuracy: 95.70%\n",
      "Batch 133/272 - Loss: 0.0992 - Accuracy: 96.88%\n",
      "Batch 134/272 - Loss: 0.0747 - Accuracy: 97.27%\n",
      "Batch 135/272 - Loss: 0.0882 - Accuracy: 96.88%\n",
      "Batch 136/272 - Loss: 0.1020 - Accuracy: 96.09%\n",
      "Batch 137/272 - Loss: 0.0991 - Accuracy: 96.09%\n",
      "Batch 138/272 - Loss: 0.0719 - Accuracy: 98.05%\n",
      "Batch 139/272 - Loss: 0.0738 - Accuracy: 98.83%\n",
      "Batch 140/272 - Loss: 0.0792 - Accuracy: 97.66%\n",
      "Batch 141/272 - Loss: 0.1048 - Accuracy: 96.09%\n",
      "Batch 142/272 - Loss: 0.0791 - Accuracy: 97.66%\n",
      "Batch 143/272 - Loss: 0.0934 - Accuracy: 97.27%\n",
      "Batch 144/272 - Loss: 0.0816 - Accuracy: 97.66%\n",
      "Batch 145/272 - Loss: 0.0882 - Accuracy: 97.66%\n",
      "Batch 146/272 - Loss: 0.0539 - Accuracy: 98.44%\n",
      "Batch 147/272 - Loss: 0.0584 - Accuracy: 98.44%\n",
      "Batch 148/272 - Loss: 0.0482 - Accuracy: 98.83%\n",
      "Batch 149/272 - Loss: 0.1034 - Accuracy: 96.48%\n",
      "Batch 150/272 - Loss: 0.0841 - Accuracy: 97.27%\n",
      "Batch 151/272 - Loss: 0.1027 - Accuracy: 96.48%\n",
      "Batch 152/272 - Loss: 0.0957 - Accuracy: 96.48%\n",
      "Batch 153/272 - Loss: 0.1014 - Accuracy: 96.48%\n",
      "Batch 154/272 - Loss: 0.1034 - Accuracy: 96.48%\n",
      "Batch 155/272 - Loss: 0.0681 - Accuracy: 97.27%\n",
      "Batch 156/272 - Loss: 0.1356 - Accuracy: 95.31%\n",
      "Batch 157/272 - Loss: 0.0760 - Accuracy: 96.88%\n",
      "Batch 158/272 - Loss: 0.0620 - Accuracy: 97.66%\n",
      "Batch 159/272 - Loss: 0.0725 - Accuracy: 97.27%\n",
      "Batch 160/272 - Loss: 0.0876 - Accuracy: 96.48%\n",
      "Batch 161/272 - Loss: 0.0899 - Accuracy: 98.05%\n",
      "Batch 162/272 - Loss: 0.0679 - Accuracy: 97.27%\n",
      "Batch 163/272 - Loss: 0.0800 - Accuracy: 97.27%\n",
      "Batch 164/272 - Loss: 0.0537 - Accuracy: 98.05%\n",
      "Batch 165/272 - Loss: 0.0970 - Accuracy: 96.48%\n",
      "Batch 166/272 - Loss: 0.0714 - Accuracy: 97.66%\n",
      "Batch 167/272 - Loss: 0.0888 - Accuracy: 98.05%\n",
      "Batch 168/272 - Loss: 0.1192 - Accuracy: 96.48%\n",
      "Batch 169/272 - Loss: 0.0807 - Accuracy: 96.48%\n",
      "Batch 170/272 - Loss: 0.0744 - Accuracy: 97.66%\n",
      "Batch 171/272 - Loss: 0.0574 - Accuracy: 98.05%\n",
      "Batch 172/272 - Loss: 0.0725 - Accuracy: 97.27%\n",
      "Batch 173/272 - Loss: 0.0887 - Accuracy: 96.48%\n",
      "Batch 174/272 - Loss: 0.0800 - Accuracy: 97.66%\n",
      "Batch 175/272 - Loss: 0.0703 - Accuracy: 98.05%\n",
      "Batch 176/272 - Loss: 0.0564 - Accuracy: 98.44%\n",
      "Batch 177/272 - Loss: 0.0739 - Accuracy: 98.05%\n",
      "Batch 178/272 - Loss: 0.1080 - Accuracy: 96.48%\n",
      "Batch 179/272 - Loss: 0.0987 - Accuracy: 96.88%\n",
      "Batch 180/272 - Loss: 0.0788 - Accuracy: 96.48%\n",
      "Batch 181/272 - Loss: 0.0774 - Accuracy: 97.66%\n",
      "Batch 182/272 - Loss: 0.0427 - Accuracy: 99.22%\n",
      "Batch 183/272 - Loss: 0.1048 - Accuracy: 96.88%\n",
      "Batch 184/272 - Loss: 0.0481 - Accuracy: 98.83%\n",
      "Batch 185/272 - Loss: 0.0937 - Accuracy: 96.09%\n",
      "Batch 186/272 - Loss: 0.0971 - Accuracy: 97.27%\n",
      "Batch 187/272 - Loss: 0.0525 - Accuracy: 98.05%\n",
      "Batch 188/272 - Loss: 0.1018 - Accuracy: 96.48%\n",
      "Batch 189/272 - Loss: 0.0530 - Accuracy: 98.05%\n",
      "Batch 190/272 - Loss: 0.0617 - Accuracy: 97.66%\n",
      "Batch 191/272 - Loss: 0.0795 - Accuracy: 97.66%\n",
      "Batch 192/272 - Loss: 0.0598 - Accuracy: 98.05%\n",
      "Batch 193/272 - Loss: 0.0568 - Accuracy: 98.05%\n",
      "Batch 194/272 - Loss: 0.0700 - Accuracy: 98.44%\n",
      "Batch 195/272 - Loss: 0.0686 - Accuracy: 97.27%\n",
      "Batch 196/272 - Loss: 0.0857 - Accuracy: 98.05%\n",
      "Batch 197/272 - Loss: 0.0625 - Accuracy: 98.44%\n",
      "Batch 198/272 - Loss: 0.0716 - Accuracy: 97.27%\n",
      "Batch 199/272 - Loss: 0.0623 - Accuracy: 97.66%\n",
      "Batch 200/272 - Loss: 0.0832 - Accuracy: 98.05%\n",
      "Batch 201/272 - Loss: 0.0760 - Accuracy: 96.88%\n",
      "Batch 202/272 - Loss: 0.0418 - Accuracy: 99.22%\n",
      "Batch 203/272 - Loss: 0.0540 - Accuracy: 98.44%\n",
      "Batch 204/272 - Loss: 0.1069 - Accuracy: 96.88%\n",
      "Batch 205/272 - Loss: 0.0890 - Accuracy: 97.27%\n",
      "Batch 206/272 - Loss: 0.0728 - Accuracy: 97.66%\n",
      "Batch 207/272 - Loss: 0.0809 - Accuracy: 96.09%\n",
      "Batch 208/272 - Loss: 0.0521 - Accuracy: 98.83%\n",
      "Batch 209/272 - Loss: 0.0478 - Accuracy: 99.22%\n",
      "Batch 210/272 - Loss: 0.0721 - Accuracy: 97.27%\n",
      "Batch 211/272 - Loss: 0.0605 - Accuracy: 98.05%\n",
      "Batch 212/272 - Loss: 0.0613 - Accuracy: 98.44%\n",
      "Batch 213/272 - Loss: 0.0842 - Accuracy: 97.27%\n",
      "Batch 214/272 - Loss: 0.0982 - Accuracy: 96.48%\n",
      "Batch 215/272 - Loss: 0.0908 - Accuracy: 95.31%\n",
      "Batch 216/272 - Loss: 0.0672 - Accuracy: 98.83%\n",
      "Batch 217/272 - Loss: 0.1119 - Accuracy: 94.53%\n",
      "Batch 218/272 - Loss: 0.0963 - Accuracy: 96.48%\n",
      "Batch 219/272 - Loss: 0.0666 - Accuracy: 97.66%\n",
      "Batch 220/272 - Loss: 0.0629 - Accuracy: 98.44%\n",
      "Batch 221/272 - Loss: 0.0935 - Accuracy: 96.88%\n",
      "Batch 222/272 - Loss: 0.0779 - Accuracy: 98.44%\n",
      "Batch 223/272 - Loss: 0.1026 - Accuracy: 95.31%\n",
      "Batch 224/272 - Loss: 0.0486 - Accuracy: 98.05%\n",
      "Batch 225/272 - Loss: 0.0628 - Accuracy: 96.88%\n",
      "Batch 226/272 - Loss: 0.0527 - Accuracy: 98.44%\n",
      "Batch 227/272 - Loss: 0.0528 - Accuracy: 98.44%\n",
      "Batch 228/272 - Loss: 0.1012 - Accuracy: 96.48%\n",
      "Batch 229/272 - Loss: 0.0367 - Accuracy: 99.22%\n",
      "Batch 230/272 - Loss: 0.0446 - Accuracy: 97.66%\n",
      "Batch 231/272 - Loss: 0.0590 - Accuracy: 98.44%\n",
      "Batch 232/272 - Loss: 0.1173 - Accuracy: 97.27%\n",
      "Batch 233/272 - Loss: 0.0922 - Accuracy: 97.27%\n",
      "Batch 234/272 - Loss: 0.0953 - Accuracy: 96.88%\n",
      "Batch 235/272 - Loss: 0.0569 - Accuracy: 97.66%\n",
      "Batch 236/272 - Loss: 0.0664 - Accuracy: 97.27%\n",
      "Batch 237/272 - Loss: 0.0592 - Accuracy: 98.05%\n",
      "Batch 238/272 - Loss: 0.0586 - Accuracy: 97.27%\n",
      "Batch 239/272 - Loss: 0.0582 - Accuracy: 98.44%\n",
      "Batch 240/272 - Loss: 0.0339 - Accuracy: 99.22%\n",
      "Batch 241/272 - Loss: 0.1079 - Accuracy: 95.70%\n",
      "Batch 242/272 - Loss: 0.1077 - Accuracy: 96.88%\n",
      "Batch 243/272 - Loss: 0.0410 - Accuracy: 99.22%\n",
      "Batch 244/272 - Loss: 0.0949 - Accuracy: 96.88%\n",
      "Batch 245/272 - Loss: 0.0765 - Accuracy: 97.66%\n",
      "Batch 246/272 - Loss: 0.1328 - Accuracy: 95.70%\n",
      "Batch 247/272 - Loss: 0.0742 - Accuracy: 98.05%\n",
      "Batch 248/272 - Loss: 0.0870 - Accuracy: 97.66%\n",
      "Batch 249/272 - Loss: 0.0903 - Accuracy: 96.88%\n",
      "Batch 250/272 - Loss: 0.1155 - Accuracy: 96.48%\n",
      "Batch 251/272 - Loss: 0.0686 - Accuracy: 97.66%\n",
      "Batch 252/272 - Loss: 0.1075 - Accuracy: 96.48%\n",
      "Batch 253/272 - Loss: 0.1130 - Accuracy: 96.09%\n",
      "Batch 254/272 - Loss: 0.0327 - Accuracy: 98.83%\n",
      "Batch 255/272 - Loss: 0.1377 - Accuracy: 96.48%\n",
      "Batch 256/272 - Loss: 0.0919 - Accuracy: 97.27%\n",
      "Batch 257/272 - Loss: 0.1043 - Accuracy: 97.27%\n",
      "Batch 258/272 - Loss: 0.0430 - Accuracy: 97.66%\n",
      "Batch 259/272 - Loss: 0.0972 - Accuracy: 96.88%\n",
      "Batch 260/272 - Loss: 0.0768 - Accuracy: 98.44%\n",
      "Batch 261/272 - Loss: 0.0843 - Accuracy: 96.09%\n",
      "Batch 262/272 - Loss: 0.0612 - Accuracy: 98.05%\n",
      "Batch 263/272 - Loss: 0.0421 - Accuracy: 99.22%\n",
      "Batch 264/272 - Loss: 0.0540 - Accuracy: 98.44%\n",
      "Batch 265/272 - Loss: 0.0671 - Accuracy: 97.66%\n",
      "Batch 266/272 - Loss: 0.0610 - Accuracy: 98.44%\n",
      "Batch 267/272 - Loss: 0.0784 - Accuracy: 97.66%\n",
      "Batch 268/272 - Loss: 0.0841 - Accuracy: 96.09%\n",
      "Batch 269/272 - Loss: 0.0975 - Accuracy: 96.48%\n",
      "Batch 270/272 - Loss: 0.0722 - Accuracy: 97.27%\n",
      "Batch 271/272 - Loss: 0.0908 - Accuracy: 96.48%\n",
      "Batch 272/272 - Loss: 0.0863 - Accuracy: 96.88%\n",
      "Training - Avg Loss: 0.0850, Accuracy: 97.21%\n",
      "Validation - Avg Loss: 0.0795, Accuracy: 97.26%\n",
      "\n",
      "\n",
      "Epoch 3/500\n",
      "Batch 1/272 - Loss: 0.0291 - Accuracy: 99.22%\n",
      "Batch 2/272 - Loss: 0.0556 - Accuracy: 98.44%\n",
      "Batch 3/272 - Loss: 0.0285 - Accuracy: 99.22%\n",
      "Batch 4/272 - Loss: 0.0634 - Accuracy: 96.48%\n",
      "Batch 5/272 - Loss: 0.0423 - Accuracy: 97.66%\n",
      "Batch 6/272 - Loss: 0.0607 - Accuracy: 97.66%\n",
      "Batch 7/272 - Loss: 0.0398 - Accuracy: 99.22%\n",
      "Batch 8/272 - Loss: 0.0388 - Accuracy: 98.83%\n",
      "Batch 9/272 - Loss: 0.0749 - Accuracy: 97.66%\n",
      "Batch 10/272 - Loss: 0.0304 - Accuracy: 98.83%\n",
      "Batch 11/272 - Loss: 0.0623 - Accuracy: 98.44%\n",
      "Batch 12/272 - Loss: 0.0539 - Accuracy: 97.66%\n",
      "Batch 13/272 - Loss: 0.0468 - Accuracy: 98.44%\n",
      "Batch 14/272 - Loss: 0.0407 - Accuracy: 97.66%\n",
      "Batch 15/272 - Loss: 0.0173 - Accuracy: 100.00%\n",
      "Batch 16/272 - Loss: 0.0302 - Accuracy: 99.22%\n",
      "Batch 17/272 - Loss: 0.0645 - Accuracy: 98.05%\n",
      "Batch 18/272 - Loss: 0.0478 - Accuracy: 98.83%\n",
      "Batch 19/272 - Loss: 0.0489 - Accuracy: 98.05%\n",
      "Batch 20/272 - Loss: 0.0661 - Accuracy: 98.05%\n",
      "Batch 21/272 - Loss: 0.0526 - Accuracy: 98.83%\n",
      "Batch 22/272 - Loss: 0.0363 - Accuracy: 99.61%\n",
      "Batch 23/272 - Loss: 0.0192 - Accuracy: 100.00%\n",
      "Batch 24/272 - Loss: 0.0197 - Accuracy: 100.00%\n",
      "Batch 25/272 - Loss: 0.0489 - Accuracy: 98.44%\n",
      "Batch 26/272 - Loss: 0.0388 - Accuracy: 99.22%\n",
      "Batch 27/272 - Loss: 0.0439 - Accuracy: 99.22%\n",
      "Batch 28/272 - Loss: 0.0618 - Accuracy: 98.44%\n",
      "Batch 29/272 - Loss: 0.0356 - Accuracy: 99.22%\n",
      "Batch 30/272 - Loss: 0.0258 - Accuracy: 99.61%\n",
      "Batch 31/272 - Loss: 0.0467 - Accuracy: 98.05%\n",
      "Batch 32/272 - Loss: 0.0512 - Accuracy: 98.44%\n",
      "Batch 33/272 - Loss: 0.0572 - Accuracy: 98.05%\n",
      "Batch 34/272 - Loss: 0.0462 - Accuracy: 98.44%\n",
      "Batch 35/272 - Loss: 0.0487 - Accuracy: 98.05%\n",
      "Batch 36/272 - Loss: 0.0701 - Accuracy: 98.44%\n",
      "Batch 37/272 - Loss: 0.0422 - Accuracy: 99.61%\n",
      "Batch 38/272 - Loss: 0.0285 - Accuracy: 99.61%\n",
      "Batch 39/272 - Loss: 0.0399 - Accuracy: 99.22%\n",
      "Batch 40/272 - Loss: 0.0525 - Accuracy: 98.44%\n",
      "Batch 41/272 - Loss: 0.0393 - Accuracy: 98.44%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    # ===== Training Phase =====\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    model.train()\n",
    "    for batch_idx, (batch_data, batch_labels) in enumerate(train_dataloader):\n",
    "        \n",
    "        batch_data = torch.Tensor(batch_data.tolist()).float().to(device)\n",
    "        batch_labels = torch.Tensor(batch_labels.tolist()).long().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch_data)\n",
    "        \n",
    "        loss = criterion(logits, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Compute Accuracy\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        batch_correct = (preds == batch_labels).sum().item()\n",
    "        batch_total = batch_labels.size(0)\n",
    "        correct += batch_correct\n",
    "        total += batch_total\n",
    "\n",
    "        # Print loss and accuracy for this batch\n",
    "        batch_acc = batch_correct / batch_total * 100  # Convert to percentage\n",
    "        print(f\"Batch {batch_idx+1}/{len(train_dataloader)} - Loss: {loss.item():.4f} - Accuracy: {batch_acc:.2f}%\")\n",
    "\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    train_accuracy = (correct / total) * 100\n",
    "    print(f\"Training - Avg Loss: {avg_train_loss:.4f}, Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "    # ===== Validation Phase =====\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        model.eval()\n",
    "        for batch_data, batch_labels in val_dataloader:\n",
    "            batch_data = torch.Tensor(batch_data.tolist()).float().to(device)\n",
    "            batch_labels = torch.Tensor(batch_labels.tolist()).long().to(device)\n",
    "\n",
    "            logits = model(batch_data)\n",
    "            loss = criterion(logits, batch_labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Compute Accuracy\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            val_correct += (preds == batch_labels).sum().item()\n",
    "            val_total += batch_labels.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    val_accuracy = (val_correct / val_total) * 100\n",
    "    print(f\"Validation - Avg Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.2f}%\\n\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b40b12-2005-4204-9e00-32e4977aa9bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
